{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Analysis and Comparison\n",
    "\n",
    "This notebook loads the pre-trained models and evaluates their performance on the test set. It performs the following steps:\n",
    "1. Loads and preprocesses the same historical data to recreate the exact same test set that the models were not trained on.\n",
    "2. Loads the trained model objects (`baseline_model.pkl` and `2hl_model.pkl`) from the `../models/` directory.\n",
    "3. Generates predictions from both models on the training, validation, and test sets.\n",
    "4. Calculates the final Mean Squared Error (MSE) for each model on each dataset.\n",
    "5. Visualizes the predictions against the actual values and presents a summary table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path to import neural_networks module\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "from neural_networks import NeuralNetwork_0hl, NeuralNetwork_2hl\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data Loading (same as in training notebook to ensure consistency)\n",
    "df_pred_raw = pd.read_excel(\"../data/AR6-SYR-LR-F2-5-Panel(a).xlsx\", sheet_name=\"Data\")\n",
    "df_pred = df_pred_raw.drop([1,2,3,4,6,7,8,10,11,13,14,16])\n",
    "df_pred = df_pred.drop(['Unnamed: 1',2019], axis=1)\n",
    "GHG_past_raw = df_pred.iloc[[0]].values[0,1:7]\n",
    "df_pred = df_pred.rename(columns={'spm_cat (year)': 'Year'})\n",
    "df_pred = df_pred.set_index('Year')\n",
    "df_pred = df_pred.transpose()\n",
    "df_pred = df_pred.drop('Past GHG emissions (Black line) ', axis=1)\n",
    "df_pred = df_pred.rename(columns={'Trend from implemented policies (Lowest bound of  red shading ) ': 'Trend from implemented policies','Limit warming to 2°C (>67%) or return warming to 1.5°C (>50%) after a high overshoot, NDCs until 2030 (Median , dark navy blue line )': 'Limit warming to 2°C or return warming to 1.5°C after a high overshoot', 'Limit warming to 2°C (>67%) (Median , dark green line )': 'Limit warming to 2°C', 'Limit warming to 1.5°C (>50%) with no or limited overshoot ( Median ligh blue line ) ': 'Limit warming to 1.5°C'})\n",
    "df_pred = df_pred.drop([2010,2011,2012,2013,2014], axis=0)\n",
    "for year in df_pred.index[:-1]:\n",
    "    diff = df_pred.loc[year+5] - df_pred.loc[year]\n",
    "    for i in range(4):\n",
    "        df_pred.loc[year+i+1] = df_pred.loc[year] + (i+1)*diff/5\n",
    "df_pred = df_pred.sort_index()\n",
    "df_past_GHG2 = pd.read_csv(\"https://ourworldindata.org/grapher/total-ghg-emissions.csv?v=1&csvType=full&useColumnShortNames=true\", storage_options = {'User-Agent': 'Our World In Data data fetch/1.0'})\n",
    "df_past_GHG2 = df_past_GHG2.loc[df_past_GHG2['Entity'] == 'World']\n",
    "df_past_GHG2 = df_past_GHG2.drop(['Entity','Code'], axis=1)\n",
    "df_past_GHG2 = df_past_GHG2.set_index('Year')\n",
    "df_past_GHG2.annual_emissions_ghg_total_co2eq *= 10**(-9)\n",
    "GHG_past_comb = df_past_GHG2.copy()\n",
    "GHG_past_comb.loc[[2012, 2013, 2014], 'annual_emissions_ghg_total_co2eq'] = [float(emi) for emi in GHG_past_raw[2:5]]\n",
    "del_years = np.arange(2015,2024)\n",
    "GHG_past_comb = GHG_past_comb.drop(del_years)\n",
    "df_sealevel = pd.read_csv(\"https://ourworldindata.org/grapher/sea-level.csv?v=1&csvType=full&useColumnShortNames=true\", storage_options = {'User-Agent': 'Our World In Data data fetch/1.0'})\n",
    "df_sealevel = df_sealevel.drop(['Entity','Code','sea_level_church_and_white_2011','sea_level_average'], axis=1)\n",
    "df_sealevel = df_sealevel.dropna()\n",
    "df_sealevel['Day'] = [np.datetime64(day) for day in df_sealevel['Day']]\n",
    "df_sealevel = df_sealevel.groupby(df_sealevel.Day.dt.year).mean()\n",
    "df_sealevel = df_sealevel.drop('Day', axis=1, errors='ignore')\n",
    "\n",
    "# Normalization \n",
    "GHG_past_norm = (GHG_past_comb - GHG_past_comb.mean()) / GHG_past_comb.std()\n",
    "sealevel_norm = (df_sealevel - df_sealevel.mean()) / df_sealevel.std()\n",
    "\n",
    "# Sequence and Splitting \n",
    "def get_GHG_sequence(n_years, df_GHG, start_year, end_year):\n",
    "    X, y = list(), list()\n",
    "    for i in range(start_year, end_year + 1):\n",
    "        end_ix = i - 1\n",
    "        start_ix = end_ix - n_years + 1\n",
    "        seq_x = df_GHG.loc[start_ix:end_ix]\n",
    "        X.append(seq_x.to_numpy())\n",
    "        y.append(sealevel_norm.loc[i].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "timespan = 15\n",
    "train_end_year = 2000\n",
    "validation_end_year = 2007\n",
    "test_end_year = 2014\n",
    "\n",
    "X_train, y_train = get_GHG_sequence(timespan, GHG_past_norm, 1970, train_end_year)\n",
    "X_val, y_val = get_GHG_sequence(timespan, GHG_past_norm, train_end_year + 1, validation_end_year)\n",
    "X_test, y_test = get_GHG_sequence(timespan, GHG_past_norm, validation_end_year + 1, test_end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models from the files\n",
    "nn_base = NeuralNetwork_0hl.load_model('../models/baseline_model.pkl')\n",
    "nn_2hl = NeuralNetwork_2hl.load_model('../models/2hl_model.pkl')\n",
    "\n",
    "print('Models loaded successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all datasets\n",
    "pred_base_train = nn_base.predict(np.squeeze(X_train))\n",
    "pred_base_val = nn_base.predict(np.squeeze(X_val))\n",
    "pred_base_test = nn_base.predict(np.squeeze(X_test))\n",
    "\n",
    "pred_2hl_train = nn_2hl.predict(np.squeeze(X_train))\n",
    "pred_2hl_val = nn_2hl.predict(np.squeeze(X_val))\n",
    "pred_2hl_test = nn_2hl.predict(np.squeeze(X_test))\n",
    "\n",
    "# Calculate MSE\n",
    "mse_base_train_final = np.mean(np.square(y_train - pred_base_train))\n",
    "mse_base_val_final = np.mean(np.square(y_val - pred_base_val))\n",
    "mse_base_test_final = np.mean(np.square(y_test - pred_base_test))\n",
    "\n",
    "mse_2hl_train_final = np.mean(np.square(y_train - pred_2hl_train))\n",
    "mse_2hl_val_final = np.mean(np.square(y_val - pred_2hl_val))\n",
    "mse_2hl_test_final = np.mean(np.square(y_test - pred_2hl_test))\n",
    "\n",
    "print('--- Baseline Model ---')\n",
    "print(f'Final MSE (Train): {mse_base_train_final:.4f}')\n",
    "print(f'Final MSE (Validation): {mse_base_val_final:.4f}')\n",
    "print(f'Final MSE (Test): {mse_base_test_final:.4f}')\n",
    "\n",
    "print('\n--- 2-Layer Model ---')\n",
    "print(f'Final MSE (Train): {mse_2hl_train_final:.4f}')\n",
    "print(f'Final MSE (Validation): {mse_2hl_val_final:.4f}')\n",
    "print(f'Final MSE (Test): {mse_2hl_test_final:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions vs actuals for the test set\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "test_years = np.arange(validation_end_year + 1, test_end_year + 1)\n",
    "\n",
    "plt.plot(test_years, y_test, label='Actual Sea Level', marker='o')\n",
    "plt.plot(test_years, pred_base_test, label='Baseline Predictions', marker='x')\n",
    "plt.plot(test_years, pred_2hl_test, label='2-Layer Predictions', marker='s')\n",
    "\n",
    "plt.title('Model Predictions vs. Actual Data on Test Set')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Normalized Sea Level')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Results Table\n",
    "results = {\n",
    "    'Model': ['Baseline (Linear)', '2-Hidden-Layer (ReLU)'],\n",
    "    'Train MSE': [mse_base_train_final, mse_2hl_train_final],\n",
    "    'Validation MSE': [mse_base_val_final, mse_2hl_val_final],\n",
    "    'Test MSE': [mse_base_test_final, mse_2hl_test_final]\n",
    "}\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Critique and Next Steps\n",
    "\n",
    "**Analysis of Results:**\n",
    "\n",
    "*(This section will be filled in after running the notebook, but we expect to see the 2-layer model outperform the baseline, indicating a non-linear relationship. We will also analyze the gap between training and test MSE to diagnose overfitting.)*\n",
    "\n",
    "**Discussion of Open Points:**\n",
    "\n",
    "*   **Overfitting:** If the 2-layer model shows a significantly lower training MSE than test MSE, it's a sign of overfitting. Future iterations should include regularization techniques like L2 regularization (weight decay) or dropout to combat this.\n",
    "*   **Batch Size:** We are currently using batch gradient descent. Switching to mini-batch gradient descent would be computationally more efficient and could lead to better convergence by escaping local minima.\n",
    "*   **Hyperparameters:** The architecture (15-8-4-1) was chosen arbitrarily. A systematic hyperparameter search (e.g., using grid search or random search) for the number of layers, number of neurons, learning rate, and epochs is a crucial next step.\n",
    "*   **Data:** The primary limitation is likely the single-feature input. To build a more powerful model, we must enrich our dataset with more features. Potential candidates include global average temperature, atmospheric CO2 concentrations, and data on volcanic and solar activity. Sourcing this data, for example from NASA's public repositories, would be the most impactful next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}