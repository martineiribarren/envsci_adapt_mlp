{"cells":[{"cell_type":"markdown","metadata":{"id":"s_sAnnff6GV5"},"source":["# 1. Model Training\n","\n","This notebook includes the training of the sea level prediction models witha baseline linear model and a 2-hidden-layer non-linear model. It performs the following steps:\n","1. Loads and preprocesses the historical GHG and sea level data.\n","2. Splits the data into training, validation, and test sets using a **chronological** split, which is appropriate for time-series data.\n","3. Initializes two models: a baseline linear model and a 2-hidden-layer non-linear model.\n","4. Trains both models on the training data.\n","5. Saves the trained model objects to the `../models/` directory for later use in analysis and prediction."]},{"cell_type":"markdown","metadata":{"id":"eUvpnNt16GV8"},"source":["### 1.1 Setup and Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"9IHOEL3YKae3"},"source":["We utilized the processed data from the data_exploration notebook and applied normalization. Subsequently, we created the training data by pairing multi-year GHG emission inputs with corresponding historical sea level rise values. We initially selected a 15-year input timespan, recognizing the inherent time delay between GHG emissions and their impact on sea level rise. This delay is primarily due to the cumulative effect of emissions on global temperature, which drives long-term processes such as the melting of ice sheets and glaciers over decades. To prepare our model for predicting future sea level changes based on projected GHG emission scenarios, we refrained from shuffling the data and instead implemented a chronological split into training, validation, and test sets. This ensures the test and validation sets represent entirely unseen data, preventing data leakage and allowing for a realistic evaluation of the model's predictive capabilities. We maintained a standard split of 70% for training, 15% for validation, and 15% for testing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y117OhuWKae3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sys\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9dy6yrKKae4"},"outputs":[],"source":["%%capture\n","from ipynb.fs.full.data_exploration import df_sealevel, GHG_past_comb, df_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3spmFrEqKae5","outputId":"84062b34-0cc3-47cf-cf9b-75e5a9579cb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size: 31\n","Validation set size: 7\n","Test set size: 7\n"]}],"source":["# Add src directory to path to import neural_networks module\n","sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n","from neural_networks import NeuralNetwork_0hl, NeuralNetwork_2hl\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","\n","# Create models directory if it doesn't exist\n","os.makedirs('../models', exist_ok=True)\n","\n","# Prepare the data for normalization\n","df_pred = df_pred.rename(columns={'Trend from implemented policies (Lowest bound of  red shading ) ': 'Trend from implemented policies','Limit warming to 2°C (>67%) or return warming to 1.5°C (>50%) after a high overshoot, NDCs until 2030 (Median , dark navy blue line )': 'Limit warming to 2°C or return warming to 1.5°C after a high overshoot', 'Limit warming to 2°C (>67%) (Median , dark green line )': 'Limit warming to 2°C', 'Limit warming to 1.5°C (>50%) with no or limited overshoot ( Median ligh blue line ) ': 'Limit warming to 1.5°C'})\n","df_sealevel = df_sealevel.groupby(df_sealevel.Day.dt.year).mean()\n","df_sealevel = df_sealevel.drop('Day', axis=1, errors='ignore')\n","\n","# Normalization\n","GHG_past_norm = (GHG_past_comb - GHG_past_comb.mean()) / GHG_past_comb.std()\n","sealevel_norm = (df_sealevel - df_sealevel.mean()) / df_sealevel.std()\n","\n","# Sequence and Splitting\n","def get_GHG_sequence(n_years, df_GHG, start_year, end_year):\n","    X, y = list(), list()\n","    for i in range(start_year, end_year + 1):\n","        end_ix = i - 1\n","        start_ix = end_ix - n_years + 1\n","        seq_x = df_GHG.loc[start_ix:end_ix]\n","        X.append(seq_x.to_numpy())\n","        y.append(sealevel_norm.loc[i].values)\n","    return np.array(X), np.array(y)\n","\n","timespan = 15\n","train_end_year = 2000\n","validation_end_year = 2007\n","test_end_year = 2014\n","\n","X_train, y_train = get_GHG_sequence(timespan, GHG_past_norm, 1970, train_end_year)\n","X_val, y_val = get_GHG_sequence(timespan, GHG_past_norm, train_end_year + 1, validation_end_year)\n","X_test, y_test = get_GHG_sequence(timespan, GHG_past_norm, validation_end_year + 1, test_end_year)\n","\n","print(f'Training set size: {len(X_train)}')\n","print(f'Validation set size: {len(X_val)}')\n","print(f'Test set size: {len(X_test)}')"]},{"cell_type":"markdown","metadata":{"id":"iDS7QSzg6GV-"},"source":["### 1.2 Train Baseline Model (Linear)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntT5gjO16GV_","outputId":"887a83f9-a5bc-46b7-be7c-9814e5614046"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Training Baseline Model ---\n","Training complete.\n","Baseline model saved to ../models/baseline_model.pkl\n"]}],"source":["print('--- Training Baseline Model ---')\n","nn_base = NeuralNetwork_0hl(input_size=timespan, output_size=1)\n","mse_base_train = nn_base.train(np.squeeze(X_train), y_train, epochs=20000, learningrate=0.001, print_output=False)\n","print('Training complete.')\n","\n","# Save the model\n","nn_base.save_model('../models/baseline_model.pkl')\n","print('Baseline model saved to ../models/baseline_model.pkl')"]},{"cell_type":"markdown","metadata":{"id":"lv6WJgaF6GV_"},"source":["### 1.3 Train 2-Hidden-Layer Model (Non-Linear)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4G2kKBVD6GV_","outputId":"d2e243db-37e8-4ba9-9b28-2d8bdba7432b"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Training 2-Hidden-Layer Model ---\n","Training complete.\n","2-layer model saved to ../models/2hl_model.pkl\n"]}],"source":["print('--- Training 2-Hidden-Layer Model ---')\n","nn_2hl = NeuralNetwork_2hl(input_size=timespan, hidden_size1=8, hidden_size2=4, output_size=1)\n","mse_2hl_train = nn_2hl.train(np.squeeze(X_train), y_train, epochs=20000, learningrate=0.001, print_output=False)\n","print('Training complete.')\n","\n","# Save the model\n","nn_2hl.save_model('../models/2hl_model.pkl')\n","print('2-layer model saved to ../models/2hl_model.pkl')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"sywi","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"}},"nbformat":4,"nbformat_minor":0}